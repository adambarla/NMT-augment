defaults:
    - _self_
    - augmenter: null # if null no augmentation is used, available: 'synonym'
    - tokenizer: bpe # available tokenizers: 'bpe', 'character', 'word'
    - model: seq2seq
    - optimizer: adamw
    - scheduler: aiayn
    - criterion: crossentropy
data:
    name: wmt14
    l1: cs # available combinations 'cs-en', 'de-en', 'fr-en', 'hi-en', 'ru-en'
    l2: en
    dir: './data'

wandb:
    entity: crutch
    project: "Data Augmentation for Neural Machine Translation"

augmenter: null # only here so null can be default (see defaults)

# attention is all you need defaults
emb_size: 512
d_ff: 2048
n_head: 8
n_layer: 6
vocab_size: 32000

batch_size: 32
# length in 1. characters: 143.16(mean), 127(50th%), 253(90th%),   414(99th%)
#           2. tokens, :   29.833(mean),  84(99th%), 146(99.9th%), 392(99.99th%); language czech, vocab size 32000
max_length: 128
lr: 0.1 # see scheduler/aiayn.yaml for the formula
epochs: 100
dropout: 0.1 # 0.1 is the default from the https://pytorch.org/tutorials/beginner/colab tutorial

subset_size: null # number of training examples selected (persistent depending on seed), if null whole dataset is used

name: null # if null, it will be assigned automatically as time
group: null # if null, it will be assigned automatically as time

seed: 42
device: null # if null auto cofiguration happens in this order based on availability: gpu, mps, cpu
pin_memory: true # turn on on gpu
prefetch_factor: 2 # how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.
num_workers: 8 # number of batches loaded in advance by each worker.
persistent_workers: true # if true, the data loader will not shut down the worker processes after a dataset has been consumed once

patience: 3 # number of epochs to wait for improvement before stopping the run