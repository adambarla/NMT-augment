defaults:
    - _self_
    - tokenizer: bpe
    - augmenter: synonym
    - model: seq2seq
    - optimizer: adamw
    - criterion: crossentropy
data:
    name: wmt14
    lang: fr-en # available 'cs-en', 'de-en', 'fr-en', 'hi-en', 'ru-en'
    dir: './data'

wandb:
    entity: crutch
    project: "Data Augmentation for Neural Machine Translation"

# attention is all you need defaults
emb_size: 512
d_ff: 2048
n_head: 8
n_layers: 6
vocab_size: 32000

batch_size: 32
max_length: 256 # 143.16(mean), 127(50th%), 253(90th%), 414.0(99th%)
lr: 0.001
epochs: 100
dropout: 0.1 # 0.1 is the default from the https://pytorch.org/tutorials/beginner/colab tutorial

subset_size: null # number of training examples selected (persistent depending on seed), if null whole dataset is used

name: null # if null, it will be assigned automatically as time
group: null # todo: change automatic selection of run group

seed: 42
device: null # if null auto cofiguration happens in this order based on availability: gpu, mps, cpu
pin_memory: true # turn on on gpu
prefetch_factor: 4 # how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. 
num_workers: 4 # number of batches loaded in advance by each worker.
persistent_workers: true # if true, the data loader will not shut down the worker processes after a dataset has been consumed once