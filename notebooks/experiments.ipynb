{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a43433-6e75-4daf-a884-58132de7c854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T05:54:58.948859Z",
     "start_time": "2024-04-02T05:54:58.936575Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c3f52c-221e-4fe7-b9b6-850a1b07a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import llist\n",
    "from utils import *\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from utils import (\n",
    "    init_wandb, \n",
    "    set_deterministic, \n",
    "    get_dataloaders, \n",
    "    get_dataset, \n",
    "    get_device, \n",
    "    init_tokenizers,\n",
    "    init_model\n",
    ")\n",
    "from omegaconf import OmegaConf\n",
    "from accelerate import Accelerator\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8faefe-01d5-4304-b461-c9783b4f49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('../notebooks')\n",
    "os.chdir('../src') # assuming that `jupyter notebook` is running in `notebooks/` folder\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc13b9d-1f33-4799-876d-285a7957e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"text.usetex\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2aa73c-e0d1-40cf-b99e-383b12c34661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"../src/conf\", job_name=\"test_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c44a7-5f76-47ed-b766-bcf55244dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f752c-436c-49ee-a180-fbc40a69179b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Hydra configuration:\\n{OmegaConf.to_yaml(cfg)}\")\n",
    "set_deterministic(cfg.seed)\n",
    "dataset = get_dataset(cfg)\n",
    "tokenizer_l1, tokenzier_l2 = init_tokenizers(cfg, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf963f53-76c4-4aa0-b17d-8c30b87058d8",
   "metadata": {},
   "source": [
    "### scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c5d24-11d7-42d0-9f50-da111759f4fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = init_model(cfg,tokenizer_l1, tokenizer_l2, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02bd853-31b6-42a3-a7d3-f4399036430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = hydra.utils.instantiate(cfg.optimizer,model.parameters(),lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f545070-35f8-4600-9baf-39c36bfce9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = hydra.utils.instantiate(cfg.scheduler,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370826e6-6e9f-4e61-bb40-b2b5579544f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = []\n",
    "for i in range(10000):\n",
    "    lrs.extend(scheduler.get_lr())\n",
    "    optimizer.step()\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2ee95-dc10-4157-9260-60a2e2fd2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.state_dict()['_step_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec9ba0-489f-40f1-b176-623d73b02b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3),dpi=200)\n",
    "\n",
    "ax.plot(np.array(lrs[:10000]))\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('lr')\n",
    "# fig.savefig('../images/learning_rate.svg', transparent=True, pad_inches=0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f8332-792b-47e4-bf42-db00846cc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x, d_model=512, warm_up_steps=4000):\n",
    "    return np.power(d_model, -0.5) * np.minimum(np.power(x, -0.5), x * np.power(warm_up_steps, -1.5))\n",
    "    \n",
    "xs = np.linspace(1,8000, 8000)\n",
    "ys = f(xs)\n",
    "\n",
    "plt.plot(xs,ys)\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a093da3-c31f-44df-a440-0f1b972886b8",
   "metadata": {},
   "source": [
    "### merging lists of strings benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f77d6-2d7a-4170-baf9-4cdcf8973a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import random\n",
    "import string\n",
    "\n",
    "n_iterations = 1000\n",
    "list_size = 1000  # Each list will have 100 elements\n",
    "n_lists = 10  # Number of lists to merge\n",
    "string_size = 100\n",
    "\n",
    "# Generate the lists\n",
    "to_merge_1 = [[''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(string_size)) for i in range(list_size)] for _ in range(n_lists)]\n",
    "to_merge_2 = list(to_merge_1)\n",
    "to_merge_3 = list(to_merge_1)\n",
    "\n",
    "\n",
    "# Test 1: Extending the big one as they come\n",
    "start_1 = time.time()\n",
    "big_list_1 = []\n",
    "for lst in to_merge_1:\n",
    "    big_list_1.extend(lst)\n",
    "time_1 = time.time() - start_1\n",
    "\n",
    "# Test 2: Save them to a list of lists and flatten at once with itertools.chain\n",
    "big_list_2 = []\n",
    "start_2 = time.time()\n",
    "for lst in to_merge_2:\n",
    "    big_list_2 += lst\n",
    "time_2 = time.time() - start_2\n",
    "\n",
    "start_3 = time.time()\n",
    "big_list_3 = list(itertools.chain.from_iterable(to_merge_3))\n",
    "time_3 = time.time() - start_3\n",
    "\n",
    "time_1, time_2, time_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b99de3-23ec-448d-b1f4-3b5723268338",
   "metadata": {},
   "source": [
    "### merging pairs of elements in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5ef73-293f-4260-a538-98a3aac31cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(el1, el2, new_tok, l=None):\n",
    "    new_l = []\n",
    "    i = 0\n",
    "    while i < len(l) - 1:\n",
    "        if l[i] == el1 and l[i+1] == el2:\n",
    "            new_l.append(new_tok)\n",
    "            i += 2  \n",
    "        else:\n",
    "            new_l.append(l[i])\n",
    "            i += 1\n",
    "    if i < len(l):  \n",
    "        new_l.append(l[-1])\n",
    "    return new_l\n",
    "\n",
    "def f2(el1, el2, new_tok,ll=None):\n",
    "    for node in ll.iternodes():\n",
    "        if node.next == None:\n",
    "            break\n",
    "        if node.value == el1 and node.next.value == el2:\n",
    "            node.value = new_tok\n",
    "            ll.remove(node.next)\n",
    "    return ll\n",
    "\n",
    "def f3(el1, el2, new_tok,l=None):\n",
    "    new_l = []\n",
    "    skip = False\n",
    "    t2 = None\n",
    "    for t1,t2 in zip(l,l[1:]):\n",
    "        if t1==el1 and t2==el2:\n",
    "            new_l.append(new_tok)\n",
    "            skip = True\n",
    "        elif skip:\n",
    "            skip = False\n",
    "        else:\n",
    "            new_l.append(t1)\n",
    "    if not skip and t2 is not None:\n",
    "        new_l.append(t2)\n",
    "    return new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb43b8d-7850-41a0-b839-125b483997ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    l = random.choices(range(1,3),k=100)\n",
    "    l1 = f1(1,2,3,list(l))\n",
    "    l2 = f2(1,2,3,llist.dllist(list(l)))\n",
    "    l3 = f3(1,2,3,list(l))\n",
    "    # l1 = l\n",
    "    for i in range(len(l1)):\n",
    "        if len(l1) != len(l2) or len(l2) != len(l3):\n",
    "            print('len mismatch')\n",
    "            print(l)\n",
    "            print(l1)\n",
    "            print(l2)\n",
    "            print(l3)\n",
    "            break\n",
    "        if l1[i] != l2[i] or l1[i] != l3[i]:\n",
    "            print(f\"el mismatch at {i}\")\n",
    "            print(l)\n",
    "            print(l1)\n",
    "            print(l2)\n",
    "            print(l3)\n",
    "            break\n",
    "        # print(f1(n,1,2,3,l1) == f2(n,1,2,3,l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931b365-c28e-4ec4-945c-7425af702fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "n=100000\n",
    "%timeit f1(1,2,3,random.choices(range(1,3),k=n))\n",
    "%timeit f2(1,2,3,llist.dllist(random.choices(range(1,3),k=n)))\n",
    "%timeit f3(1,2,3,random.choices(range(1,3),k=n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a29cc-ebd9-430c-a162-ca6f973b7987",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefe808-c7bf-40e8-b52c-48eb5d3ce9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'cs-en'\n",
    "l1 = lang[:2]\n",
    "l2 = lang[3:]\n",
    "l1,l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618166c1-f812-406f-afe8-af6fac72b1d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T05:54:59.844402Z",
     "start_time": "2024-04-02T05:54:58.949653Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wmt14\", lang,cache_dir='../src/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e0835-cd6a-4768-b43a-4042ca8612cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T05:45:44.928281Z",
     "start_time": "2024-04-02T05:45:44.928205Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1f7a4-348c-4788-8781-44e4013fa782",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for e in dataset['train']['translation']:\n",
    "    if len(e[l1]) < 10000:\n",
    "        lengths.append(len(tokenizer_l1.encode(e[l1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2986f5c-9733-4059-ac9c-565cb9f2cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,3),dpi=200)\n",
    "\n",
    "ax.hist(lengths, bins=100)\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('counts')\n",
    "ax.set_xlabel('length (token)')\n",
    "ax.set_title(f'sentence lengths (lang={l1}, vocab_size={32000})')\n",
    "fig.savefig('../images/sentence_lengths_hist.svg', transparent=True, pad_inches=0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64cb3ba-3e53-4dd9-bca2-409b010a9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,1))\n",
    "ax.boxplot(lengths, vert=False,widths=1)\n",
    "ax.spines[:].set_visible(False)\n",
    "ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d169048-9ca2-4938-978a-567f7cf52c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(lengths)\n",
    "# qs = 1 - np.logspace(-4,0,10)\n",
    "qs = [0.25,0.50,0.75,0.99,0.999,0.9999]\n",
    "for q in qs:\n",
    "    print(f\"quantile {q:.4f}: {np.quantile(arr,q)}\")\n",
    "arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d3e81-b17c-495c-af55-5fc89e3af8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.searchsorted(np.sort(arr), 64) / len(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1938f7-1615-4b08-898b-597049503665",
   "metadata": {},
   "source": [
    "## hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80014d4a-d72c-480c-8633-a0c541ce4c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "\n",
    "_hash(dataset,1000,0.01,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756cfe64-0862-4cad-b113-e0b8f45f6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for partition in dataset.values():\n",
    "    for example in tqdm(partition):\n",
    "        for l_example in example['translation'].values():\n",
    "            s = l_example.encode('utf-8')\n",
    "            lengths.append(len(s))\n",
    "arr = np.array(lengths)\n",
    "arr.mean(), np.quantile(arr,0.50), np.quantile(arr,0.90), np.quantile(arr,0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5db41d-301f-4405-8b4d-3e56a484aa01",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec507fef-eebe-4f38-8da6-ad1d659247d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chars = set()\n",
    "for i in dataset:\n",
    "    for r in dataset[i]:\n",
    "        for t in r['translation']:\n",
    "            chars.update(r['translation'][t])\n",
    "chars = sorted(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9717df4-6e24-4f38-9f37-5b0ad81c551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "encode = lambda x: [stoi.get(c, stoi[\"a\"]) for c in x]\n",
    "decode = lambda x: ''.join([itos[c] for c in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14346464-4b32-46ab-b8b4-2e938450fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4b8ff-5a37-448f-ac48-7a0db8c19586",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    display(pd.DataFrame(itos.values(),itos.keys()).iloc[i*20:(i+1)*20].T)\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d0cdc-d896-4df4-94ad-1dc1d8c25321",
   "metadata": {},
   "source": [
    "### bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2145eb-c946-4c52-968e-3e5f120f18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset['train']['translation'][0]\n",
    "example = example[cfg.data.l1] + \"\\n\" + example[cfg.data.l2]\n",
    "print(f\"{tokenizer_l1.lang} tok:\")\n",
    "print(colorize_tokens(tokenizer_l1.encode(example), tokenizer_l1))\n",
    "print(f\"{tokenizer_l2.lang} tok:\")\n",
    "print(colorize_tokens(tokenizer_l2.encode(example), tokenizer_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9a626-742b-468f-86ec-d85d0afbea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PersistentRandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28236d-8911-49ba-b015-f12111810c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PersistentRandom(5)\n",
    "pr.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e1055-a91a-4e77-ad5b-e20ca84f60d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init(dataset, fraction=0.01):\n",
    "    l = list()\n",
    "    t = set()\n",
    "    space = ' '.encode('utf-8')\n",
    "    for key, partition in dataset.items():\n",
    "        with tqdm(total=len(partition), desc=f\"bpe: consolidating {key} partition\") as pbar:\n",
    "            for example in partition:\n",
    "                if pr.rand() < fraction:\n",
    "                    for l_example in example['translation'].values():\n",
    "                        s = l_example.encode('utf-8')\n",
    "                        t.update(s)\n",
    "                        l.extend(list(s)+list(space))\n",
    "                pbar.update(1)\n",
    "        return l,t    \n",
    "l,t = init(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c9562-39a8-47d8-b9b4-ba07bc4ea084",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t), len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a8566-0922-4183-88d6-d6983b58c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_pair_counts(token_list):\n",
    "    pair_counts = Counter()\n",
    "    pair_counts.update(zip(token_list, token_list[1:]))\n",
    "    return pair_counts     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cda749-d156-4dd5-95f3-d586c45291e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(lst, pair, new_elem):\n",
    "    elem1, elem2 = pair\n",
    "    i = 0\n",
    "    new_list = []\n",
    "    while i < len(lst) - 1:\n",
    "        if lst[i] == elem1 and lst[i+1] == elem2:\n",
    "            new_list.append(new_elem)\n",
    "            i += 2 \n",
    "        else:\n",
    "            new_list.append(lst[i])\n",
    "            i += 1\n",
    "    if i < len(lst):\n",
    "        new_list.append(lst[i])  \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221bf2d-b06d-4538-88b7-5926487144b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 400\n",
    "new_tok = 256\n",
    "token_dict = {key: bytes([key]) for key in t}\n",
    "merges = dict()\n",
    "for i in tqdm(range(max_tokens - len(t))):\n",
    "    counts = get_pair_counts(l)\n",
    "    to_merge = counts.most_common(1)[0][0]\n",
    "    l = merge(l, to_merge, new_tok)\n",
    "    token_dict[new_tok] = token_dict[to_merge[0]] + token_dict[to_merge[1]]\n",
    "    merges[to_merge] = new_tok\n",
    "    new_tok += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a60abb-a48f-4ebc-99ad-943c1e7086db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_dict), len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c2321-e219-4cdb-a7ae-f2f3e60f2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict[252] = b'<unk>'\n",
    "token_dict[253] = b'<s/>'\n",
    "token_dict[254] = b'<pad>'\n",
    "token_dict[255] = b'<s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff13c02-61e9-4467-a8ff-8e82ccdaee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04596570-92f4-4dc7-9d77-4a051462a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x, token_dict,truncation=True, add_special_tokens=True, padding=\"max_length\", max_length=20):\n",
    "    encoded = list(x.encode('utf-8'))\n",
    "    while len(encoded) > 1:\n",
    "        pairs = get_pair_counts(encoded)\n",
    "        to_merge = min(pairs, key=lambda k: merges.get(k, float('inf')))\n",
    "        if to_merge not in merges:\n",
    "            break\n",
    "        replace = merges[to_merge]\n",
    "        encoded = merge(encoded, to_merge, replace)\n",
    "    if truncation and max_length is not None:\n",
    "        encoded = encoded[: max_length - (2 if add_special_tokens else 0)]\n",
    "    if add_special_tokens:\n",
    "        encoded = [255] + encoded + [253]\n",
    "    if padding == \"max_length\" and max_length is not None:\n",
    "        encoded += [254] * (max_length - len(encoded))\n",
    "    return encoded\n",
    "\n",
    "def decode(x, vocab):\n",
    "    special_token_ids = [252, 253, 254, 255]\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.tolist()\n",
    "    if isinstance(x, list) and (not x or isinstance(x[0], int)):\n",
    "        x = [x]\n",
    "    decoded_text = [\n",
    "        b\"\".join(vocab.get(t,b'\\xef\\xbf\\xbd') for t in seq if t not in special_token_ids).decode('utf-8', errors='replace')\n",
    "        for seq in x\n",
    "    ]\n",
    "    return decoded_text\n",
    "\n",
    "print(encode(\"Hej kámo, čo ti práši?\",token_dict))\n",
    "decode(encode(\"Hej kámo, čo ti práši?\",token_dict)+[127],token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081402e-1dbf-4b21-8052-a7f192b9c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(' ',token_dict,add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7c447-4795-49a6-af94-9873445ae6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'�'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ccfe1-16f8-4be4-b01b-cc50e62cf15b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list of invalid utf-8 bytes\n",
    "invalid_bytes = []\n",
    "for i in range(256):\n",
    "    errors = 0\n",
    "    try:\n",
    "        bytes([i]).decode('utf-8')\n",
    "    except:\n",
    "        errors += 1\n",
    "    try:\n",
    "        bytes([i,128]).decode('utf-8')\n",
    "    except:\n",
    "        errors += 1\n",
    "    try:\n",
    "        bytes([i,128,128]).decode('utf-8')\n",
    "    except:\n",
    "        errors += 1\n",
    "    try:\n",
    "        bytes([i,128,128,128]).decode('utf-8')\n",
    "    except:\n",
    "        errors += 1\n",
    "    try:\n",
    "        bytes([11000000,i]).decode('utf-8')\n",
    "    except:\n",
    "        errors += 1\n",
    "    if errors == 5:\n",
    "        invalid_bytes.append(i)\n",
    "np.array(invalid_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a24fad-25df-4438-96b5-2c10dae97378",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01406d-d2f4-440c-982c-c3c8d3aaefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hydra.utils.instantiate(cfg.model, device=device, \n",
    "                                pad_token_id=tokenizer.pad_token_id,\n",
    "                                bos_token_id=tokenizer.bos_token_id,\n",
    "                                eos_token_id=tokenizer.eos_token_id,)\n",
    "model.to(device)\n",
    "print(f\"Model:\\n{model}\")\n",
    "optimizer = hydra.utils.instantiate(cfg.optimizer, model.parameters())\n",
    "print(f\"Optimizer:\\n{optimizer}\")\n",
    "criterion = hydra.utils.instantiate(cfg.criterion)\n",
    "print(f\"Criterion:\\n{criterion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "inputs, targets = next(iter(train_loader))\n",
    "inputs = inputs.transpose(0,1).to(device)\n",
    "targets = targets.transpose(0,1).to(device)\n",
    "\n",
    "outputs = model(inputs, targets[:-1, :])\n",
    "print(outputs.isnan().sum().item())\n",
    "print(outputs.shape)\n",
    "print(targets.shape)\n",
    "loss = criterion(outputs.reshape(-1, outputs.shape[-1]), targets[1:,:].reshape(-1))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dea959-b861-4f82-9ad8-18e3bcd096dd",
   "metadata": {},
   "source": [
    "## pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe64a3-1007-46ad-8e88-f698a39727cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876740a-a022-44d8-98ff-84e214c599b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "input_ids = torch.tensor(tokenizer.encode(\"<mask>\")).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80260132-41ae-4a3d-8d37-af2f399ab7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([0,1,1,1,1,200,200,2],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b65c52-490d-4fc7-9c0b-21f4233c58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c1cb1-7346-4940-9a20-6036d1df8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloaders(cfg,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6f1c2-a2c7-4d07-bf4c-5caaa2254623",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e1dff-3c2a-4758-a910-eaad6778c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = 0\n",
    "optimizer = hydra.utils.instantiate(cfg.optimizer,params=model.parameters())\n",
    "criterion = \n",
    "model.train()\n",
    "for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input, output = batch\n",
    "    predictions = model(input)\n",
    "\n",
    "    loss = criterion(predictions, batch.label)\n",
    "    # if regularizer is not None:\n",
    "    #     loss += regularizer(model)\n",
    "    # loss.backward()\n",
    "    # if grad_clip_threshold is not None:\n",
    "    #     torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_threshold)\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12928cbc-5336-4a71-a6ce-3cd0c584e64c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
