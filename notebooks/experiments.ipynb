{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6e06b-3652-4613-8ec4-11afb189d0f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T05:54:58.948859Z",
     "start_time": "2024-04-02T05:54:58.936575Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da637a-dc5a-4ad2-accd-0a26f4fc6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import llist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b99de3-23ec-448d-b1f4-3b5723268338",
   "metadata": {},
   "source": [
    "### merging pairs of elements in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5ef73-293f-4260-a538-98a3aac31cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(el1, el2, new_tok, l=None):\n",
    "    new_l = []\n",
    "    i = 0\n",
    "    while i < len(l) - 1:\n",
    "        if l[i] == el1 and l[i+1] == el2:\n",
    "            new_l.append(new_tok)\n",
    "            i += 2  \n",
    "        else:\n",
    "            new_l.append(l[i])\n",
    "            i += 1\n",
    "    if i < len(l):  \n",
    "        new_l.append(l[-1])\n",
    "    return new_l\n",
    "\n",
    "def f2(el1, el2, new_tok,ll=None):\n",
    "    for node in ll.iternodes():\n",
    "        if node.next == None:\n",
    "            break\n",
    "        if node.value == el1 and node.next.value == el2:\n",
    "            node.value = new_tok\n",
    "            ll.remove(node.next)\n",
    "    return ll\n",
    "\n",
    "def f3(el1, el2, new_tok,l=None):\n",
    "    new_l = []\n",
    "    skip = False\n",
    "    t2 = None\n",
    "    for t1,t2 in zip(l,l[1:]):\n",
    "        if t1==el1 and t2==el2:\n",
    "            new_l.append(new_tok)\n",
    "            skip = True\n",
    "        elif skip:\n",
    "            skip = False\n",
    "        else:\n",
    "            new_l.append(t1)\n",
    "    if not skip and t2 is not None:\n",
    "        new_l.append(t2)\n",
    "    return new_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb43b8d-7850-41a0-b839-125b483997ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    l = random.choices(range(1,3),k=100)\n",
    "    l1 = f1(1,2,3,list(l))\n",
    "    l2 = f2(1,2,3,llist.dllist(list(l)))\n",
    "    l3 = f3(1,2,3,list(l))\n",
    "    # l1 = l\n",
    "    for i in range(len(l1)):\n",
    "        if len(l1) != len(l2) or len(l2) != len(l3):\n",
    "            print('len mismatch')\n",
    "            print(l)\n",
    "            print(l1)\n",
    "            print(l2)\n",
    "            print(l3)\n",
    "            break\n",
    "        if l1[i] != l2[i] or l1[i] != l3[i]:\n",
    "            print(f\"el mismatch at {i}\")\n",
    "            print(l)\n",
    "            print(l1)\n",
    "            print(l2)\n",
    "            print(l3)\n",
    "            break\n",
    "        # print(f1(n,1,2,3,l1) == f2(n,1,2,3,l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9931b365-c28e-4ec4-945c-7425af702fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "n=100000\n",
    "%timeit f1(1,2,3,random.choices(range(1,3),k=n))\n",
    "%timeit f2(1,2,3,llist.dllist(random.choices(range(1,3),k=n)))\n",
    "%timeit f3(1,2,3,random.choices(range(1,3),k=n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a29cc-ebd9-430c-a162-ca6f973b7987",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefe808-c7bf-40e8-b52c-48eb5d3ce9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'cs-en'\n",
    "l1 = lang[:2]\n",
    "l2 = lang[3:]\n",
    "l1,l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618166c1-f812-406f-afe8-af6fac72b1d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T05:54:59.844402Z",
     "start_time": "2024-04-02T05:54:58.949653Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wmt14\", lang,cache_dir='../src/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e0835-cd6a-4768-b43a-4042ca8612cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T05:45:44.928281Z",
     "start_time": "2024-04-02T05:45:44.928205Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1938f7-1615-4b08-898b-597049503665",
   "metadata": {},
   "source": [
    "## hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80014d4a-d72c-480c-8633-a0c541ce4c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "\n",
    "_hash(dataset,1000,0.01,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756cfe64-0862-4cad-b113-e0b8f45f6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for partition in dataset.values():\n",
    "    for example in tqdm(partition):\n",
    "        for l_example in example['translation'].values():\n",
    "            s = l_example.encode('utf-8')\n",
    "            lengths.append(len(s))\n",
    "arr = np.array(lengths)\n",
    "arr.mean(), np.quantile(arr,0.50), np.quantile(arr,0.90), np.quantile(arr,0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5db41d-301f-4405-8b4d-3e56a484aa01",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec507fef-eebe-4f38-8da6-ad1d659247d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chars = set()\n",
    "for i in dataset:\n",
    "    for r in dataset[i]:\n",
    "        for t in r['translation']:\n",
    "            chars.update(r['translation'][t])\n",
    "chars = sorted(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9717df4-6e24-4f38-9f37-5b0ad81c551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "encode = lambda x: [stoi.get(c, stoi[\"a\"]) for c in x]\n",
    "decode = lambda x: ''.join([itos[c] for c in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14346464-4b32-46ab-b8b4-2e938450fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4b8ff-5a37-448f-ac48-7a0db8c19586",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    display(pd.DataFrame(itos.values(),itos.keys()).iloc[i*20:(i+1)*20].T)\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d0cdc-d896-4df4-94ad-1dc1d8c25321",
   "metadata": {},
   "source": [
    "### bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9a626-742b-468f-86ec-d85d0afbea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PersistentRandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28236d-8911-49ba-b015-f12111810c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PersistentRandom(5)\n",
    "pr.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e1055-a91a-4e77-ad5b-e20ca84f60d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init(dataset, fraction=0.01):\n",
    "    l = list()\n",
    "    t = set()\n",
    "    space = ' '.encode('utf-8')\n",
    "    for key, partition in dataset.items():\n",
    "        with tqdm(total=len(partition), desc=f\"bpe: consolidating {key} partition\") as pbar:\n",
    "            for example in partition:\n",
    "                if pr.rand() < fraction:\n",
    "                    for l_example in example['translation'].values():\n",
    "                        s = l_example.encode('utf-8')\n",
    "                        t.update(s)\n",
    "                        l.extend(list(s)+list(space))\n",
    "                pbar.update(1)\n",
    "        return l,t    \n",
    "l,t = init(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c9562-39a8-47d8-b9b4-ba07bc4ea084",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t), len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a8566-0922-4183-88d6-d6983b58c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_pair_counts(token_list):\n",
    "    pair_counts = Counter()\n",
    "    pair_counts.update(zip(token_list, token_list[1:]))\n",
    "    return pair_counts     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cda749-d156-4dd5-95f3-d586c45291e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(lst, pair, new_elem):\n",
    "    elem1, elem2 = pair\n",
    "    i = 0\n",
    "    new_list = []\n",
    "    while i < len(lst) - 1:\n",
    "        if lst[i] == elem1 and lst[i+1] == elem2:\n",
    "            new_list.append(new_elem)\n",
    "            i += 2 \n",
    "        else:\n",
    "            new_list.append(lst[i])\n",
    "            i += 1\n",
    "    if i < len(lst):\n",
    "        new_list.append(lst[i])  \n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221bf2d-b06d-4538-88b7-5926487144b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 400\n",
    "new_tok = 256\n",
    "token_dict = {key: bytes([key]) for key in t}\n",
    "merges = dict()\n",
    "for i in tqdm(range(max_tokens - len(t))):\n",
    "    counts = get_pair_counts(l)\n",
    "    to_merge = counts.most_common(1)[0][0]\n",
    "    l = merge(l, to_merge, new_tok)\n",
    "    token_dict[new_tok] = token_dict[to_merge[0]] + token_dict[to_merge[1]]\n",
    "    merges[to_merge] = new_tok\n",
    "    new_tok += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a60abb-a48f-4ebc-99ad-943c1e7086db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_dict), len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c2321-e219-4cdb-a7ae-f2f3e60f2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict[252] = b'<unk>'\n",
    "token_dict[253] = b'<s/>'\n",
    "token_dict[254] = b'<pad>'\n",
    "token_dict[255] = b'<s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff13c02-61e9-4467-a8ff-8e82ccdaee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dae05c-b3e0-4f69-8842-bea38f6de914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04596570-92f4-4dc7-9d77-4a051462a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x, token_dict,truncation=True, add_special_tokens=True, padding=\"max_length\", max_length=20):\n",
    "    encoded = list(x.encode('utf-8'))\n",
    "    while len(encoded) > 1:\n",
    "        pairs = get_pair_counts(encoded)\n",
    "        to_merge = min(pairs, key=lambda k: merges.get(k, float('inf')))\n",
    "        if to_merge not in merges:\n",
    "            break\n",
    "        replace = merges[to_merge]\n",
    "        encoded = merge(encoded, to_merge, replace)\n",
    "    if truncation and max_length is not None:\n",
    "        encoded = encoded[: max_length - (2 if add_special_tokens else 0)]\n",
    "    if add_special_tokens:\n",
    "        encoded = [255] + encoded + [253]\n",
    "    if padding == \"max_length\" and max_length is not None:\n",
    "        encoded += [254] * (max_length - len(encoded))\n",
    "    return encoded\n",
    "\n",
    "def decode(x, vocab):\n",
    "    special_token_ids = [252, 253, 254, 255]\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.tolist()\n",
    "    if isinstance(x, list) and (not x or isinstance(x[0], int)):\n",
    "        x = [x]\n",
    "    decoded_text = [\n",
    "        b\"\".join(vocab.get(t,b'\\xef\\xbf\\xbd') for t in seq if t not in special_token_ids).decode('utf-8', errors='replace')\n",
    "        for seq in x\n",
    "    ]\n",
    "    return decoded_text\n",
    "\n",
    "print(encode(\"Hej kámo, čo ti práši?\",token_dict))\n",
    "decode(encode(\"Hej kámo, čo ti práši?\",token_dict)+[127],token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081402e-1dbf-4b21-8052-a7f192b9c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(' ',token_dict,add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7c447-4795-49a6-af94-9873445ae6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'�'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ccfe1-16f8-4be4-b01b-cc50e62cf15b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list of invalid utf-8 bytes\n",
    "invalid_bytes = []\n",
    "for i in range(256):\n",
    "    errors = 0\n",
    "    try:\n",
    "        bytes([i]).decode('utf-8')\n",
    "    except:\n",
    "        errors += 1\n",
    "    try:\n",
    "        bytes([i,128]).decode('utf-8')\n",
    "    except:\n",
    "        errors += 1\n",
    "    try:\n",
    "        bytes([i,128,128]).decode('utf-8')\n",
    "    except:\n",
    "        errors += 1\n",
    "    try:\n",
    "        bytes([i,128,128,128]).decode('utf-8')\n",
    "    except:\n",
    "        errors += 1\n",
    "    try:\n",
    "        bytes([11000000,i]).decode('utf-8')\n",
    "    except:\n",
    "        errors += 1\n",
    "    if errors == 5:\n",
    "        invalid_bytes.append(i)\n",
    "np.array(invalid_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a24fad-25df-4438-96b5-2c10dae97378",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c3f52c-221e-4fe7-b9b6-850a1b07a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import init_wandb, set_deterministic, get_dataloaders, get_dataset, get_device\n",
    "from omegaconf import OmegaConf\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2aa73c-e0d1-40cf-b99e-383b12c34661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initialize(version_base=None, config_path=\"../src/conf\", job_name=\"test_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f752c-436c-49ee-a180-fbc40a69179b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"main\",overrides=['data.dir=../src/data'])\n",
    "print(f\"Hydra configuration:\\n{OmegaConf.to_yaml(cfg)}\")\n",
    "device = get_device(cfg)\n",
    "dataset = get_dataset(cfg)\n",
    "tokenizer = hydra.utils.instantiate(cfg.tokenizer, dataset=dataset)\n",
    "cfg.src_vocab_size = tokenizer.vocab_size\n",
    "cfg.tgt_vocab_size = tokenizer.vocab_size\n",
    "print(f\"Tokenizer:\\n{tokenizer}\")\n",
    "train_loader, val_loader, test_loader = get_dataloaders(cfg, tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01406d-d2f4-440c-982c-c3c8d3aaefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hydra.utils.instantiate(cfg.model, device=device, \n",
    "                                pad_token_id=tokenizer.pad_token_id,\n",
    "                                bos_token_id=tokenizer.bos_token_id,\n",
    "                                eos_token_id=tokenizer.eos_token_id,)\n",
    "model.to(device)\n",
    "print(f\"Model:\\n{model}\")\n",
    "optimizer = hydra.utils.instantiate(cfg.optimizer, model.parameters())\n",
    "print(f\"Optimizer:\\n{optimizer}\")\n",
    "criterion = hydra.utils.instantiate(cfg.criterion)\n",
    "print(f\"Criterion:\\n{criterion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "inputs, targets = next(iter(train_loader))\n",
    "inputs = inputs.transpose(0,1).to(device)\n",
    "targets = targets.transpose(0,1).to(device)\n",
    "\n",
    "outputs = model(inputs, targets[:-1, :])\n",
    "print(outputs.isnan().sum().item())\n",
    "print(outputs.shape)\n",
    "print(targets.shape)\n",
    "loss = criterion(outputs.reshape(-1, outputs.shape[-1]), targets[1:,:].reshape(-1))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dea959-b861-4f82-9ad8-18e3bcd096dd",
   "metadata": {},
   "source": [
    "## pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fe64a3-1007-46ad-8e88-f698a39727cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876740a-a022-44d8-98ff-84e214c599b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "model = XLMRobertaModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "input_ids = torch.tensor(tokenizer.encode(\"<mask>\")).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80260132-41ae-4a3d-8d37-af2f399ab7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([0,1,1,1,1,200,200,2],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b65c52-490d-4fc7-9c0b-21f4233c58cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"<pad>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c1cb1-7346-4940-9a20-6036d1df8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloaders(cfg,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d6f1c2-a2c7-4d07-bf4c-5caaa2254623",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e1dff-3c2a-4758-a910-eaad6778c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = 0\n",
    "optimizer = hydra.utils.instantiate(cfg.optimizer,params=model.parameters())\n",
    "criterion = \n",
    "model.train()\n",
    "for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input, output = batch\n",
    "    predictions = model(input)\n",
    "\n",
    "    loss = criterion(predictions, batch.label)\n",
    "    # if regularizer is not None:\n",
    "    #     loss += regularizer(model)\n",
    "    # loss.backward()\n",
    "    # if grad_clip_threshold is not None:\n",
    "    #     torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_threshold)\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0b1e8-9778-42be-bb67-3ae52bf056ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
